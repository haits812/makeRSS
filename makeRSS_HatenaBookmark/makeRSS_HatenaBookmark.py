import requests
import re
import xml.etree.ElementTree as ET
from xml.dom import minidom
import os
import csv

MAX_XML_ITEMS = 300  # XMLã«ä¿æŒã™ã‚‹æœ€å¤§ã‚¢ã‚¤ãƒ†ãƒ æ•°

def load_existing_csv(csv_file):
    """CSVãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰æ—¢å­˜ã®ã‚¢ã‚¤ãƒ†ãƒ ã‚’èª­ã¿è¾¼ã‚€"""
    existing_items = []
    existing_links = set()
    if os.path.exists(csv_file):
        with open(csv_file, 'r', encoding='utf-8-sig', newline='') as f:
            reader = csv.DictReader(f)
            for row in reader:
                existing_items.append(row)
                existing_links.add(row['link'])
    return existing_items, existing_links

def migrate_xml_to_csv(xml_file, csv_file):
    """æ—¢å­˜ã®XMLã‹ã‚‰CSVã«ãƒ‡ãƒ¼ã‚¿ã‚’ç§»è¡Œã™ã‚‹ï¼ˆåˆå›ã®ã¿ï¼‰"""
    if os.path.exists(csv_file) or not os.path.exists(xml_file):
        return [], set()
    
    print(f"Migrating data from {xml_file} to {csv_file}...")
    items = []
    links = set()
    
    try:
        tree = ET.parse(xml_file)
        root = tree.getroot()
        for item in root.findall(".//item"):
            title_elem = item.find("title")
            link_elem = item.find("link")
            desc_elem = item.find("description")
            date_elem = item.find("pubDate")
            
            if title_elem is not None and link_elem is not None:
                item_data = {
                    'title': title_elem.text or '',
                    'link': link_elem.text or '',
                    'description': desc_elem.text if desc_elem is not None else '',
                    'pubDate': date_elem.text if date_elem is not None else ''
                }
                items.append(item_data)
                links.add(item_data['link'])
        print(f"Migrated {len(items)} items from XML")
    except Exception as e:
        print(f"Error migrating XML: {e}")
    
    return items, links

def save_csv(csv_file, items):
    """å…¨ã‚¢ã‚¤ãƒ†ãƒ ã‚’CSVã«ä¿å­˜"""
    if not items:
        return
    fieldnames = ['title', 'link', 'description', 'pubDate']
    with open(csv_file, 'w', encoding='utf-8-sig', newline='') as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()
        writer.writerows(items)

def main():
    print("ã‚¹ã‚¯ãƒªãƒ—ãƒˆé–‹å§‹ï¼")
    
    # åˆæœŸè¨­å®š
    url = "https://b.hatena.ne.jp/entrylist/it/AI%E3%83%BB%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92"
    output_file = "makeRSS_HatenaBookmark.xml"
    csv_file = "makeRSS_HatenaBookmark.csv"

    print(f"åˆæœŸURL: {url}")

    # CSVãŒç„¡ã‘ã‚Œã°æ—¢å­˜XMLã‹ã‚‰ç§»è¡Œ
    if not os.path.exists(csv_file) and os.path.exists(output_file):
        all_items, existing_links = migrate_xml_to_csv(output_file, csv_file)
    else:
        # æ—¢å­˜ã®CSVã‹ã‚‰ã‚¢ã‚¤ãƒ†ãƒ ã‚’èª­ã¿è¾¼ã‚€
        all_items, existing_links = load_existing_csv(csv_file)
    
    print(f"æ—¢å­˜ã‚¢ã‚¤ãƒ†ãƒ æ•°: {len(all_items)}")

    # åˆæœŸãƒšãƒ¼ã‚¸ç•ªå·ã¨æœ€çµ‚ãƒšãƒ¼ã‚¸ç•ªå·
    start_page = 1
    end_page = 5
    current_page = start_page
    new_items = []

    # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å‡¦ç†
    while url and current_page <= end_page:
        print(f"ç¾åœ¨ã®ãƒšãƒ¼ã‚¸ï¼š{current_page}")
        
        response = requests.get(url)
        print(f"HTTPã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰: {response.status_code}")
        
        if response.status_code != 200:
            print("ãƒªã‚¯ã‚¨ã‚¹ãƒˆå¤±æ•—ï¼ğŸ˜±")
            break
            
        html_content = response.text

        article_pattern = re.compile(r'<h3 class="entrylist-contents-title">[\s\S]*?<a href="([^"]+)"[\s\S]*?title="([^"]+)"[\s\S]*?<\/a>[\s\S]*?<li class="entrylist-contents-date">([^<]+)<\/li>[\s\S]*?<p class="entrylist-contents-description" data-gtm-click-label="entry-info-description-href">([\s\S]+?)<\/p>')
    
        for match in article_pattern.findall(html_content):
            link, title, date, description = match

            if link in existing_links:
                continue

            new_item = {
                'title': title,
                'link': link,
                'description': description,
                'pubDate': date
            }
            new_items.append(new_item)
            existing_links.add(link)

        # æ¬¡ã®ãƒšãƒ¼ã‚¸ã¸
        next_page_match = re.search(r'<a href="(/entrylist/it/AI%E3%83%BB%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92\?page=\d+)" class="js-keyboard-openable">', html_content)

        if next_page_match:
            url = 'https://b.hatena.ne.jp' + next_page_match.group(1)
        else:
            url = None

        current_page += 1

    print(f"æ–°è¦ã‚¢ã‚¤ãƒ†ãƒ æ•°: {len(new_items)}")

    # æ–°ã—ã„ã‚¢ã‚¤ãƒ†ãƒ ã‚’å…ˆé ­ã«è¿½åŠ ï¼ˆCSVã¯å…¨ä»¶ä¿æŒï¼‰
    all_items = new_items + all_items
    
    # CSVã«å…¨ä»¶ä¿å­˜
    save_csv(csv_file, all_items)
    print(f"CSVä¿å­˜å®Œäº†: {len(all_items)} items")
    
    # XMLã¯æœ€æ–°500ä»¶ã®ã¿
    xml_items = all_items[:MAX_XML_ITEMS]

    # XMLã‚’ç”Ÿæˆ
    root = ET.Element("rss", version="2.0")
    channel = ET.SubElement(root, "channel")
    ET.SubElement(channel, "title").text = "ã¯ã¦ãªãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯ AIãƒ»æ©Ÿæ¢°å­¦ç¿’ã‹ã‚‰ã®æƒ…å ±"
    ET.SubElement(channel, "description").text = "ã¯ã¦ãªãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯ AIãƒ»æ©Ÿæ¢°å­¦ç¿’ã‹ã‚‰ã®æƒ…å ±ã‚’æä¾›ã—ã¾ã™ã€‚"
    ET.SubElement(channel, "link").text = "https://b.hatena.ne.jp/entrylist/it/AI%E3%83%BB%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92"

    for item_data in xml_items:
        item = ET.SubElement(channel, "item")
        ET.SubElement(item, "title").text = item_data['title']
        ET.SubElement(item, "link").text = item_data['link']
        ET.SubElement(item, "pubDate").text = item_data['pubDate']
        ET.SubElement(item, "description").text = item_data['description']

    xml_str = ET.tostring(root)
    xml_pretty_str = minidom.parseString(xml_str).toprettyxml(indent="  ")
    xml_pretty_str = os.linesep.join([s for s in xml_pretty_str.splitlines() if s.strip()])
    
    with open(output_file, "w", encoding='utf-8') as f:
        f.write(xml_pretty_str)

    print(f"XMLä¿å­˜å®Œäº†: {len(xml_items)} items (æœ€å¤§{MAX_XML_ITEMS}ä»¶)")
    print("ã‚¹ã‚¯ãƒªãƒ—ãƒˆçµ‚äº†ï¼")

if __name__ == "__main__":
    main()
